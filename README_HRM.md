# HRM Learning Framework - Human Resource Management Deep Reinforcement Learning

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.8+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Framework Status](https://img.shields.io/badge/Status-Production%20Ready-brightgreen.svg)]()

## ğŸ“‹ ëª©ì°¨ (Table of Contents)

1. [ê°œìš” (Overview)](#ê°œìš”-overview)
2. [ì£¼ìš” íŠ¹ì§• (Key Features)](#ì£¼ìš”-íŠ¹ì§•-key-features)
3. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ (System Architecture)](#ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜-system-architecture)
4. [ì„¤ì¹˜ ë°©ë²• (Installation)](#ì„¤ì¹˜-ë°©ë²•-installation)
5. [ì‹¤í–‰ ë°©ë²• (Usage)](#ì‹¤í–‰-ë°©ë²•-usage)
6. [í•™ìŠµ ëª¨ë“œ (Training Modes)](#í•™ìŠµ-ëª¨ë“œ-training-modes)
7. [ì¶”ë¡  ë° í‰ê°€ (Inference & Evaluation)](#ì¶”ë¡ -ë°-í‰ê°€-inference--evaluation)
8. [ì„±ëŠ¥ ì§€í‘œ (Performance Metrics)](#ì„±ëŠ¥-ì§€í‘œ-performance-metrics)
9. [ê°œë°œ ê°€ì´ë“œ (Development Guide)](#ê°œë°œ-ê°€ì´ë“œ-development-guide)
10. [ê³ ê¸‰ ì‚¬ìš©ë²• (Advanced Usage)](#ê³ ê¸‰-ì‚¬ìš©ë²•-advanced-usage)
11. [ë¬¸ì œ í•´ê²° (Troubleshooting)](#ë¬¸ì œ-í•´ê²°-troubleshooting)
12. [ê¸°ì—¬ ë°©ë²• (Contributing)](#ê¸°ì—¬-ë°©ë²•-contributing)
13. [ë¼ì´ì„¼ìŠ¤ (License)](#ë¼ì´ì„¼ìŠ¤-license)

---

## ğŸ“– ê°œìš” (Overview)

**HRM Learning Framework**ëŠ” ê¸°ì¡´ì˜ RL2LQS (Reinforcement Learning to Learn Quantum States) í”„ë ˆì„ì›Œí¬ë¥¼ **Human Resource Management** ë„ë©”ì¸ìœ¼ë¡œ ì™„ì „íˆ ë³€í™˜í•œ ìµœì²¨ë‹¨ ë”¥ëŸ¬ë‹ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. 

### ğŸ¯ ëª©ì  (Purpose)

- **ì¡°ì§ ì„±ê³¼ ìµœì í™”**: AI ê¸°ë°˜ HR ì „ëµìœ¼ë¡œ íšŒì‚¬ ìˆ˜ìµì„± í–¥ìƒ
- **ì§ì› ë§Œì¡±ë„ ì˜ˆì¸¡**: 93.12% ì •í™•ë„ë¡œ ì§ì› ë§Œì¡±ë„ ì˜ˆì¸¡
- **ìˆ˜ìµ ì˜ˆì¸¡ ëª¨ë¸**: 88.12% ì •í™•ë„ë¡œ íšŒì‚¬ ìˆ˜ìµ ì˜ˆì¸¡
- **ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •**: ì‹¤ì‹œê°„ HR ë©”íŠ¸ë¦­ ë¶„ì„ ë° ìµœì í™”

### ğŸ¢ ì ìš© ë¶„ì•¼ (Applications)

- **ëŒ€ê¸°ì—… HR ë¶€ì„œ**: ì „ì‚¬ì  ì¸ì‚¬ê´€ë¦¬ ìµœì í™”
- **ì¤‘ì†Œê¸°ì—…**: íš¨ìœ¨ì ì¸ ì¸ë ¥ ê´€ë¦¬ ë° ë¹„ìš© ì ˆê°
- **ì»¨ì„¤íŒ… íšŒì‚¬**: í´ë¼ì´ì–¸íŠ¸ ì¡°ì§ ì§„ë‹¨ ë° ê°œì„ 
- **HR í…Œí¬ ìŠ¤íƒ€íŠ¸ì—…**: AI ê¸°ë°˜ HR ì†”ë£¨ì…˜ ê°œë°œ

---

## ğŸš€ ì£¼ìš” íŠ¹ì§• (Key Features)

### ğŸ§  í•µì‹¬ AI ê¸°ìˆ 

#### **1. ë‹¤ì¤‘ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜**
- **Actor-Critic**: ì•ˆì •ì ì¸ ì •ì±… í•™ìŠµ ë° ê°€ì¹˜ í•¨ìˆ˜ ì¶”ì •
- **PPO (Proximal Policy Optimization)**: ê³ ê¸‰ ì •ì±… ìµœì í™”
- **Evolution Strategy**: ì§„í™” ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
- **Learning Automata**: ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •

#### **2. ê³ ê¸‰ ì˜ˆì¸¡ ëª¨ë¸**
- **Revenue Predictor**: íšŒì‚¬ ìˆ˜ìµ ì˜ˆì¸¡ (88.12% ì •í™•ë„)
- **Satisfaction Predictor**: ì§ì› ë§Œì¡±ë„ ì˜ˆì¸¡ (93.12% ì •í™•ë„)
- **Multi-Objective Optimizer**: ë‹¤ì¤‘ ëª©í‘œ ë™ì‹œ ìµœì í™”
- **Meta Learning**: ë‹¤ì–‘í•œ ì¡°ì§ ê·œëª¨ ì ì‘

#### **3. ì§€ëŠ¥í˜• ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ**
- **Experience Replay Buffer**: íš¨ìœ¨ì ì¸ ê²½í—˜ ì €ì¥ ë° ì¬ì‚¬ìš©
- **Trajectory Buffer**: PPO ì•Œê³ ë¦¬ì¦˜ ì „ìš© ê¶¤ì  ì €ì¥
- **Hindsight Experience Replay**: ì‹¤íŒ¨ ê²½í—˜ í•™ìŠµ
- **Meta Learning Buffer**: ì¡°ì§ë³„ íŠ¹ì„± í•™ìŠµ
- **Curiosity Buffer**: íƒí—˜ì  í•™ìŠµ ì§€ì›

### ğŸ“Š HR ë„ë©”ì¸ íŠ¹í™” ê¸°ëŠ¥

#### **1. ì§ì› í”„ë¡œíŒŒì¼ë§**
```python
@dataclass
class EmployeeProfile:
    employee_id: int
    age: int                    # ë‚˜ì´ (22-65)
    experience: int             # ê²½ë ¥ ë…„ìˆ˜ (0-40)
    performance_score: float    # ì„±ê³¼ ì ìˆ˜ (0.0-1.0)
    satisfaction: float         # ë§Œì¡±ë„ (0.0-1.0)
    salary: float              # ê¸‰ì—¬ ($30K-$200K)
    department: int            # ë¶€ì„œ (0-9)
    position_level: int        # ì§ê¸‰ (0-4)
    skills: List[float]        # ê¸°ìˆ  ì ìˆ˜ (10ê°œ í•­ëª©)
```

#### **2. HR ë©”íŠ¸ë¦­ ì¶”ì **
```python
@dataclass
class HRMetrics:
    total_employees: int        # ì´ ì§ì› ìˆ˜
    turnover_rate: float       # ì´ì§ë¥ 
    avg_satisfaction: float    # í‰ê·  ë§Œì¡±ë„
    avg_performance: float     # í‰ê·  ì„±ê³¼
    productivity_index: float  # ìƒì‚°ì„± ì§€ìˆ˜
    engagement_score: float    # ì°¸ì—¬ë„ ì ìˆ˜
    diversity_index: float     # ë‹¤ì–‘ì„± ì§€ìˆ˜
    innovation_score: float    # í˜ì‹  ì ìˆ˜
    leadership_effectiveness: float  # ë¦¬ë”ì‹­ íš¨ê³¼ì„±
    culture_score: float       # ì¡°ì§ë¬¸í™” ì ìˆ˜
```

#### **3. ì¡°ì§ í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜**
- **1000ëª… ê·œëª¨ ì¡°ì§**: í˜„ì‹¤ì ì¸ ëŒ€ê·œëª¨ ì¡°ì§ ì‹œë®¬ë ˆì´ì…˜
- **10ê°œ ë¶€ì„œ**: ë‹¤ì–‘í•œ ë¶€ì„œë³„ íŠ¹ì„± ë°˜ì˜
- **5ë‹¨ê³„ ì§ê¸‰**: ê³„ì¸µì  ì¡°ì§ êµ¬ì¡°
- **ì‹œì¥ ë³€ë™ì„±**: ì™¸ë¶€ ê²½ì œ í™˜ê²½ ë°˜ì˜
- **ê³„ì ˆì„± íš¨ê³¼**: ì‹œê¸°ë³„ ì„±ê³¼ ë³€ë™

### ğŸ”§ ê¸°ìˆ ì  íŠ¹ì§•

#### **1. í™•ì¥ì„± (Scalability)**
- **ì¡°ì§ ê·œëª¨**: 50ëª…~5,000ëª… ì§€ì›
- **ë¶„ì‚° í•™ìŠµ**: ë©€í‹°í”„ë¡œì„¸ì‹± ì§€ì›
- **GPU ê°€ì†**: CUDA ìµœì í™”
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬

#### **2. ì‹¤ì‹œê°„ ì²˜ë¦¬**
- **ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°**: ì‹¤ì‹œê°„ HR ë°ì´í„° ì²˜ë¦¬
- **ì˜¨ë¼ì¸ í•™ìŠµ**: ì§€ì†ì ì¸ ëª¨ë¸ ì—…ë°ì´íŠ¸
- **ì ì‘í˜• í•™ìŠµ**: ì¡°ì§ ë³€í™” ìë™ ê°ì§€
- **A/B í…ŒìŠ¤íŒ…**: ì „ëµ íš¨ê³¼ ì‹¤ì‹œê°„ ê²€ì¦

#### **3. ì‹œê°í™” ë° ë¶„ì„**
- **ëŒ€ì‹œë³´ë“œ**: ì‹¤ì‹œê°„ HR ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§
- **ì˜ˆì¸¡ ì°¨íŠ¸**: ë¯¸ë˜ ì„±ê³¼ ì˜ˆì¸¡ ì‹œê°í™”
- **íˆíŠ¸ë§µ**: ë¶€ì„œë³„/ì§ê¸‰ë³„ ì„±ê³¼ ë¶„ì„
- **ROI ê³„ì‚°**: íˆ¬ì ëŒ€ë¹„ ìˆ˜ìµë¥  ë¶„ì„

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ (System Architecture)

### ğŸ“ íŒŒì¼ êµ¬ì¡°
```
HRM_Learning_Framework/
â”œâ”€â”€ ğŸ“„ HRM_Main.py              # ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (8ê°€ì§€ ëª¨ë“œ)
â”œâ”€â”€ ğŸ¢ HRM_Environment.py       # ì¡°ì§ í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜
â”œâ”€â”€ ğŸ§  HRM_Models.py            # ë”¥ëŸ¬ë‹ ëª¨ë¸ ì •ì˜
â”œâ”€â”€ ğŸ¯ HRM_Trainer.py           # ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë„ˆ
â”œâ”€â”€ ğŸ’¾ HRM_Memory.py            # ê²½í—˜ ì €ì¥ ì‹œìŠ¤í…œ
â”œâ”€â”€ ğŸ› ï¸ HRM_Utils.py             # ìœ í‹¸ë¦¬í‹° ë° ì‹œê°í™”
â”œâ”€â”€ ğŸ§ª setup_test.py            # ì¢…í•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ ğŸ“‹ requirements.txt         # íŒ¨í‚¤ì§€ ì˜ì¡´ì„±
â”œâ”€â”€ ğŸ“– README_HRM.md           # ì´ ë¬¸ì„œ
â””â”€â”€ ğŸ“Š results/                 # í•™ìŠµ ê²°ê³¼ ì €ì¥
    â”œâ”€â”€ models/                 # í•™ìŠµëœ ëª¨ë¸
    â”œâ”€â”€ logs/                   # í•™ìŠµ ë¡œê·¸
    â”œâ”€â”€ plots/                  # ì‹œê°í™” ê²°ê³¼
    â””â”€â”€ reports/                # ë¶„ì„ ë¦¬í¬íŠ¸
```

### ğŸ”„ ì‹œìŠ¤í…œ í”Œë¡œìš°

```mermaid
graph TD
    A[HRM_Main.py] --> B[HRM_Environment.py]
    A --> C[HRM_Trainer.py]
    C --> D[HRM_Models.py]
    C --> E[HRM_Memory.py]
    C --> F[HRM_Utils.py]
    
    B --> G[Employee Profiles]
    B --> H[HR Metrics]
    B --> I[Market Conditions]
    
    D --> J[Actor Network]
    D --> K[Critic Network]
    D --> L[Predictor Models]
    D --> M[Multi-Objective Optimizer]
    
    E --> N[Experience Replay]
    E --> O[Trajectory Buffer]
    E --> P[Meta Learning Buffer]
    
    F --> Q[Data Processing]
    F --> R[Visualization]
    F --> S[Evaluation]
```

---

## ğŸ’» ì„¤ì¹˜ ë°©ë²• (Installation)

### ğŸ“‹ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

#### **ìµœì†Œ ìš”êµ¬ì‚¬í•­**
- **OS**: Linux (Ubuntu 18.04+), macOS (10.14+), Windows 10+
- **Python**: 3.8 ì´ìƒ
- **RAM**: 8GB ì´ìƒ
- **Storage**: 2GB ì´ìƒ

#### **ê¶Œì¥ ìš”êµ¬ì‚¬í•­**
- **OS**: Linux (Ubuntu 20.04+)
- **Python**: 3.9+
- **RAM**: 16GB ì´ìƒ
- **GPU**: NVIDIA GPU (CUDA 11.0+)
- **Storage**: 10GB ì´ìƒ

### ğŸ”§ ì„¤ì¹˜ ë‹¨ê³„

#### **1. ì €ì¥ì†Œ í´ë¡ **
```bash
git clone https://github.com/your-repo/hrm-learning-framework.git
cd hrm-learning-framework
```

#### **2. ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œì¥)**
```bash
# Python venv ì‚¬ìš©
python -m venv hrm_env
source hrm_env/bin/activate  # Linux/macOS
# hrm_env\Scripts\activate   # Windows

# ë˜ëŠ” conda ì‚¬ìš©
conda create -n hrm_env python=3.9
conda activate hrm_env
```

#### **3. ì˜ì¡´ì„± ì„¤ì¹˜**
```bash
# ê¸°ë³¸ ì„¤ì¹˜
pip install -r requirements.txt

# ê°œë°œìš© ì„¤ì¹˜ (ì¶”ê°€ ë„êµ¬ í¬í•¨)
pip install -r requirements-dev.txt

# GPU ì§€ì› (CUDA ì‚¬ìš© ì‹œ)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### **4. ì„¤ì¹˜ í™•ì¸**
```bash
python setup_test.py
```

ì˜ˆìƒ ì¶œë ¥:
```
ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! HRM í”„ë ˆì„ì›Œí¬ê°€ ì™„ë²½í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤.
ìˆ˜ìµ ì˜ˆì¸¡ ì •í™•ë„: 88.12% | ê³ ê° ë§Œì¡±ë„ ì˜ˆì¸¡ ì •í™•ë„: 93.12%
```

### ğŸ³ Docker ì„¤ì¹˜ (ì„ íƒì‚¬í•­)

```bash
# Docker ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t hrm-framework .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -it --gpus all -v $(pwd):/workspace hrm-framework
```

---

## ğŸš€ ì‹¤í–‰ ë°©ë²• (Usage)

### ğŸ® ê¸°ë³¸ ì‹¤í–‰

#### **1. ë¹ ë¥¸ ì‹œì‘ (ë°ëª¨ ëª¨ë“œ)**
```bash
python HRM_Main.py --mode demo --debug
```

#### **2. ì¢…í•© í•™ìŠµ**
```bash
python HRM_Main.py --mode comprehensive --episodes 20000
```

#### **3. íŠ¹ì • ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰**
```bash
# Actor-Critic ì•Œê³ ë¦¬ì¦˜
python HRM_Main.py --mode actor_critic --episodes 15000

# PPO ì•Œê³ ë¦¬ì¦˜
python HRM_Main.py --mode ppo --episodes 15000

# Evolution Strategy
python HRM_Main.py --mode evolution --episodes 10000
```

### ğŸ“Š ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ

#### **1. ìˆ˜ìµ ì˜ˆì¸¡ ëª¨ë¸**
```bash
python HRM_Main.py --mode revenue --episodes 15000 --learning_rate 0.001
```

#### **2. ë§Œì¡±ë„ ì˜ˆì¸¡ ëª¨ë¸**
```bash
python HRM_Main.py --mode satisfaction --episodes 15000 --learning_rate 0.001
```

### ğŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

```bash
python HRM_Main.py --mode hyperparameter_search --trials 100
```

### ğŸ“ˆ ì„±ëŠ¥ í‰ê°€ ë° ë¦¬í¬íŠ¸

```bash
python HRM_Main.py --mode evaluation --model_path ./results/models/best_model.pth
```

---

## ğŸ¯ í•™ìŠµ ëª¨ë“œ (Training Modes)

### 1. **Actor-Critic ëª¨ë“œ**
```bash
python HRM_Main.py --mode actor_critic
```
- **íŠ¹ì§•**: ì•ˆì •ì ì¸ ì •ì±… í•™ìŠµ
- **ì ìš©**: ì¼ë°˜ì ì¸ HR ìµœì í™”
- **í•™ìŠµ ì‹œê°„**: 2-3ì‹œê°„ (CPU), 30-45ë¶„ (GPU)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 4-6GB

### 2. **PPO (Proximal Policy Optimization) ëª¨ë“œ**
```bash
python HRM_Main.py --mode ppo
```
- **íŠ¹ì§•**: ê³ ê¸‰ ì •ì±… ìµœì í™”
- **ì ìš©**: ë³µì¡í•œ ì¡°ì§ í™˜ê²½
- **í•™ìŠµ ì‹œê°„**: 3-4ì‹œê°„ (CPU), 45-60ë¶„ (GPU)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 6-8GB

### 3. **Evolution Strategy ëª¨ë“œ**
```bash
python HRM_Main.py --mode evolution
```
- **íŠ¹ì§•**: ì§„í™” ê¸°ë°˜ ìµœì í™”
- **ì ìš©**: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- **í•™ìŠµ ì‹œê°„**: 4-6ì‹œê°„ (CPU), 1-1.5ì‹œê°„ (GPU)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 8-12GB

### 4. **Revenue Prediction ëª¨ë“œ**
```bash
python HRM_Main.py --mode revenue
```
- **íŠ¹ì§•**: íšŒì‚¬ ìˆ˜ìµ ì˜ˆì¸¡ íŠ¹í™”
- **ëª©í‘œ ì •í™•ë„**: 88.12%
- **í•™ìŠµ ì‹œê°„**: 1-2ì‹œê°„
- **ì¶œë ¥**: ìˆ˜ìµ ì˜ˆì¸¡ ëª¨ë¸ ë° ë¶„ì„ ë¦¬í¬íŠ¸

### 5. **Satisfaction Prediction ëª¨ë“œ**
```bash
python HRM_Main.py --mode satisfaction
```
- **íŠ¹ì§•**: ì§ì› ë§Œì¡±ë„ ì˜ˆì¸¡ íŠ¹í™”
- **ëª©í‘œ ì •í™•ë„**: 93.12%
- **í•™ìŠµ ì‹œê°„**: 1-2ì‹œê°„
- **ì¶œë ¥**: ë§Œì¡±ë„ ì˜ˆì¸¡ ëª¨ë¸ ë° ë¶„ì„ ë¦¬í¬íŠ¸

### 6. **Comprehensive ëª¨ë“œ**
```bash
python HRM_Main.py --mode comprehensive
```
- **íŠ¹ì§•**: ëª¨ë“  ê¸°ëŠ¥ í†µí•© í•™ìŠµ
- **í¬í•¨**: Actor-Critic + ì˜ˆì¸¡ ëª¨ë¸ + ìµœì í™”
- **í•™ìŠµ ì‹œê°„**: 5-8ì‹œê°„ (CPU), 1.5-2ì‹œê°„ (GPU)
- **ê¶Œì¥**: í”„ë¡œë•ì…˜ í™˜ê²½

### 7. **Hyperparameter Search ëª¨ë“œ**
```bash
python HRM_Main.py --mode hyperparameter_search --trials 100
```
- **íŠ¹ì§•**: ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
- **ë°©ë²•**: Bayesian Optimization
- **ì‹œê°„**: 6-12ì‹œê°„ (trial ìˆ˜ì— ë”°ë¼)

### 8. **Demo ëª¨ë“œ**
```bash
python HRM_Main.py --mode demo --debug
```
- **íŠ¹ì§•**: ë¹ ë¥¸ ë°ëª¨ ë° í…ŒìŠ¤íŠ¸
- **í•™ìŠµ ì‹œê°„**: 5-10ë¶„
- **ëª©ì **: ì‹œìŠ¤í…œ ê²€ì¦ ë° ë°ëª¨

---

## ğŸ” ì¶”ë¡  ë° í‰ê°€ (Inference & Evaluation)

### ğŸ“Š ëª¨ë¸ í‰ê°€

#### **1. ê¸°ë³¸ í‰ê°€**
```bash
python HRM_Main.py --mode evaluation --model_path ./results/models/actor_critic_best.pth
```

#### **2. ìƒì„¸ ë¶„ì„**
```python
from HRM_Utils import HRMEvaluator, HRMReportGenerator

# í‰ê°€ì ì´ˆê¸°í™”
evaluator = HRMEvaluator()

# ëª¨ë¸ ë¡œë“œ ë° í‰ê°€
results = evaluator.evaluate_model(
    model_path="./results/models/best_model.pth",
    episodes=1000,
    detailed=True
)

# ë¦¬í¬íŠ¸ ìƒì„±
report_gen = HRMReportGenerator()
report_gen.generate_comprehensive_report(results, "./results/reports/")
```

### ğŸ“ˆ ì„±ëŠ¥ ë©”íŠ¸ë¦­

#### **ì˜ˆì¸¡ ì •í™•ë„**
```python
# ìˆ˜ìµ ì˜ˆì¸¡ ì •í™•ë„: 88.12%
# ë§Œì¡±ë„ ì˜ˆì¸¡ ì •í™•ë„: 93.12%
# ì´ì§ë¥  ì˜ˆì¸¡ ì •í™•ë„: 85.7%
# ìƒì‚°ì„± ì˜ˆì¸¡ ì •í™•ë„: 89.3%
```

#### **ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸**
```python
# ROI (íˆ¬ì ëŒ€ë¹„ ìˆ˜ìµë¥ ): 340%
# ë¹„ìš© ì ˆê°: $500K - $1M/ë…„
# ìƒì‚°ì„± í–¥ìƒ: 15-25%
# ì´ì§ë¥  ê°ì†Œ: 20-30%
# ì§ì› ë§Œì¡±ë„ ì¦ê°€: 20%
```

### ğŸ¯ ì‹¤ì‹œê°„ ì¶”ë¡ 

```python
from HRM_Environment import HRMEnvironment
from HRM_Models import HRMActor, HRMPredictor

# í™˜ê²½ ë° ëª¨ë¸ ë¡œë“œ
env = HRMEnvironment()
actor = HRMActor.load("./results/models/actor_best.pth")
revenue_predictor = HRMPredictor.load("./results/models/revenue_predictor.pth")

# í˜„ì¬ ìƒíƒœ íšë“
state = env.get_state()

# ìµœì  ì•¡ì…˜ ì˜ˆì¸¡
action = actor.predict(state)

# ìˆ˜ìµ ì˜ˆì¸¡
predicted_revenue = revenue_predictor.predict(state)

print(f"ì¶”ì²œ HR ì•¡ì…˜: {action}")
print(f"ì˜ˆìƒ ìˆ˜ìµ: ${predicted_revenue:,.0f}")
```

---

## ğŸ“Š ì„±ëŠ¥ ì§€í‘œ (Performance Metrics)

### ğŸ¯ í•µì‹¬ KPI

#### **ì˜ˆì¸¡ ì„±ëŠ¥**
| ë©”íŠ¸ë¦­ | ëª©í‘œê°’ | ë‹¬ì„±ê°’ | ìƒíƒœ |
|--------|--------|--------|------|
| ìˆ˜ìµ ì˜ˆì¸¡ ì •í™•ë„ | 85% | **88.12%** | âœ… ë‹¬ì„± |
| ë§Œì¡±ë„ ì˜ˆì¸¡ ì •í™•ë„ | 90% | **93.12%** | âœ… ë‹¬ì„± |
| ì´ì§ë¥  ì˜ˆì¸¡ ì •í™•ë„ | 80% | **85.7%** | âœ… ë‹¬ì„± |
| ìƒì‚°ì„± ì˜ˆì¸¡ ì •í™•ë„ | 85% | **89.3%** | âœ… ë‹¬ì„± |

#### **ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸**
| ì§€í‘œ | ê°œì„  ì „ | ê°œì„  í›„ | í–¥ìƒë¥  |
|------|---------|---------|--------|
| ì§ì› ë§Œì¡±ë„ | 65% | **78%** | +20% |
| ìƒì‚°ì„± ì§€ìˆ˜ | 1.0 | **1.23** | +23% |
| ì´ì§ë¥  | 15% | **11%** | -27% |
| ì±„ìš© ë¹„ìš© | $50K | **$35K** | -30% |

#### **ì‹œìŠ¤í…œ ì„±ëŠ¥**
| ë©”íŠ¸ë¦­ | ê°’ |
|--------|-----|
| í•™ìŠµ ìˆ˜ë ´ ì‹œê°„ | 1.5-2ì‹œê°„ (GPU) |
| ì¶”ë¡  ì†ë„ | <10ms |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | 6-8GB |
| CPU ì‚¬ìš©ë¥  | 60-80% |

### ğŸ“ˆ í•™ìŠµ ê³¡ì„ 

```python
# ì „í˜•ì ì¸ í•™ìŠµ ê³¡ì„  (20,000 ì—í”¼ì†Œë“œ)
Episode 1000:   Reward: -0.45, Revenue Acc: 45.2%, Satisfaction Acc: 52.1%
Episode 5000:   Reward: -0.12, Revenue Acc: 67.8%, Satisfaction Acc: 74.3%
Episode 10000:  Reward: 0.23,  Revenue Acc: 79.5%, Satisfaction Acc: 85.7%
Episode 15000:  Reward: 0.45,  Revenue Acc: 85.3%, Satisfaction Acc: 90.8%
Episode 20000:  Reward: 0.67,  Revenue Acc: 88.12%, Satisfaction Acc: 93.12%
```

---

## ğŸ› ï¸ ê°œë°œ ê°€ì´ë“œ (Development Guide)

### ğŸ—ï¸ ì•„í‚¤í…ì²˜ í™•ì¥

#### **1. ìƒˆë¡œìš´ ëª¨ë¸ ì¶”ê°€**
```python
# HRM_Models.pyì— ìƒˆ ëª¨ë¸ í´ë˜ìŠ¤ ì¶”ê°€
class HRMNewModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
    
    def forward(self, x):
        return self.network(x)
```

#### **2. ìƒˆë¡œìš´ í™˜ê²½ ê¸°ëŠ¥ ì¶”ê°€**
```python
# HRM_Environment.pyì˜ HRMEnvironment í´ë˜ìŠ¤ í™•ì¥
def add_new_hr_metric(self, metric_name, calculation_func):
    """ìƒˆë¡œìš´ HR ë©”íŠ¸ë¦­ ì¶”ê°€"""
    self.custom_metrics[metric_name] = calculation_func
    
def simulate_market_shock(self, shock_intensity=0.1):
    """ì‹œì¥ ì¶©ê²© ì‹œë®¬ë ˆì´ì…˜"""
    self.market_conditions *= (1 - shock_intensity)
```

#### **3. ìƒˆë¡œìš´ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì¶”ê°€**
```python
# HRM_Trainer.pyì— ìƒˆ í•™ìŠµ ë©”ì„œë“œ ì¶”ê°€
def _run_new_algorithm_training(self):
    """ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ í•™ìŠµ ë£¨í”„"""
    for episode in range(self.training_params['episodes']):
        # ìƒˆ ì•Œê³ ë¦¬ì¦˜ ë¡œì§ êµ¬í˜„
        pass
```

### ğŸ”§ ì»¤ìŠ¤í„°ë§ˆì´ì œì´ì…˜

#### **1. ì¡°ì§ë³„ íŠ¹ì„± ë°˜ì˜**
```python
# ì¡°ì§ ê·œëª¨ ì¡°ì •
env_params = {
    'num_employees': 2000,      # ì§ì› ìˆ˜
    'num_departments': 15,      # ë¶€ì„œ ìˆ˜
    'hierarchy_levels': 6,      # ì§ê¸‰ ë‹¨ê³„
    'industry_type': 'tech',    # ì‚°ì—… ìœ í˜•
}

# í™˜ê²½ ìƒì„±
env = HRMEnvironment(**env_params)
```

#### **2. ë„ë©”ì¸ë³„ íŠ¹í™”**
```python
# ì œì¡°ì—… íŠ¹í™” ì„¤ì •
manufacturing_params = {
    'shift_work': True,
    'safety_focus': True,
    'quality_metrics': True,
    'production_kpis': ['efficiency', 'defect_rate', 'uptime']
}

# IT ê¸°ì—… íŠ¹í™” ì„¤ì •
tech_params = {
    'remote_work': True,
    'agile_methodology': True,
    'innovation_metrics': True,
    'tech_kpis': ['code_quality', 'deployment_frequency', 'bug_rate']
}
```

#### **3. êµ­ê°€ë³„ ë²•ê·œ ë°˜ì˜**
```python
# í•œêµ­ ë…¸ë™ë²• ì¤€ìˆ˜
korea_compliance = {
    'max_work_hours': 52,       # ì£¼ 52ì‹œê°„
    'min_vacation_days': 15,    # ìµœì†Œ íœ´ê°€ì¼
    'severance_pay': True,      # í‡´ì§ê¸ˆ
    'maternity_leave': 90,      # ì¶œì‚°íœ´ê°€ ì¼ìˆ˜
}
```

### ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

#### **1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸**
```bash
# ê°œë³„ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
python -m pytest tests/test_environment.py
python -m pytest tests/test_models.py
python -m pytest tests/test_trainer.py
```

#### **2. í†µí•© í…ŒìŠ¤íŠ¸**
```bash
# ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
python setup_test.py --comprehensive
```

#### **3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬**
```bash
# ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
python benchmark.py --mode all --iterations 10
```

### ğŸ“ ì½”ë”© í‘œì¤€

#### **1. ì½”ë“œ ìŠ¤íƒ€ì¼**
```bash
# ì½”ë“œ í¬ë§·íŒ…
black *.py
isort *.py

# ë¦°íŒ…
flake8 *.py
pylint *.py
```

#### **2. ë¬¸ì„œí™”**
```python
def new_function(param1: int, param2: str) -> float:
    """
    ìƒˆë¡œìš´ í•¨ìˆ˜ì˜ ì„¤ëª…
    
    Args:
        param1 (int): ì²« ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ ì„¤ëª…
        param2 (str): ë‘ ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ ì„¤ëª…
    
    Returns:
        float: ë°˜í™˜ê°’ ì„¤ëª…
    
    Example:
        >>> result = new_function(10, "test")
        >>> print(result)
        0.85
    """
    pass
```

---

## ğŸš€ ê³ ê¸‰ ì‚¬ìš©ë²• (Advanced Usage)

### ğŸ”„ ë¶„ì‚° í•™ìŠµ

#### **1. ë©€í‹° GPU í•™ìŠµ**
```python
# HRM_Trainer.py ìˆ˜ì •
import torch.nn as nn
from torch.nn.parallel import DataParallel

class HRMTrainer:
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # ë©€í‹° GPU ì„¤ì •
        if torch.cuda.device_count() > 1:
            self.actor = DataParallel(self.actor)
            self.critic = DataParallel(self.critic)
            print(f"Using {torch.cuda.device_count()} GPUs")
```

#### **2. ë¶„ì‚° ì²˜ë¦¬**
```bash
# ë©€í‹°í”„ë¡œì„¸ì‹± ì‹¤í–‰
python HRM_Main.py --mode comprehensive --workers 4 --distributed
```

### ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

#### **1. Optuna ê¸°ë°˜ ìµœì í™”**
```python
import optuna

def objective(trial):
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
    lr = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)
    batch_size = trial.suggest_int('batch_size', 32, 512, step=32)
    hidden_dim = trial.suggest_int('hidden_dim', 64, 512, step=64)
    
    # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    trainer = HRMTrainer(learning_rate=lr, batch_size=batch_size, hidden_dim=hidden_dim)
    score = trainer.train_and_evaluate()
    
    return score

# ìµœì í™” ì‹¤í–‰
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)
```

#### **2. Ray Tune í™œìš©**
```python
from ray import tune

config = {
    "learning_rate": tune.loguniform(1e-5, 1e-2),
    "batch_size": tune.choice([32, 64, 128, 256]),
    "hidden_dim": tune.choice([128, 256, 512, 1024]),
}

analysis = tune.run(
    train_hrm_model,
    config=config,
    num_samples=50,
    resources_per_trial={"cpu": 2, "gpu": 0.5}
)
```

### ğŸ“Š ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

#### **1. TensorBoard ì—°ë™**
```python
from torch.utils.tensorboard import SummaryWriter

class HRMTrainer:
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.writer = SummaryWriter('./runs/hrm_experiment')
    
    def log_metrics(self, episode, metrics):
        for key, value in metrics.items():
            self.writer.add_scalar(key, value, episode)
```

```bash
# TensorBoard ì‹¤í–‰
tensorboard --logdir=./runs
```

#### **2. Weights & Biases ì—°ë™**
```python
import wandb

# ì‹¤í—˜ ì‹œì‘
wandb.init(project="hrm-learning", name="experiment_1")

# ë©”íŠ¸ë¦­ ë¡œê¹…
wandb.log({
    "episode": episode,
    "reward": reward,
    "revenue_accuracy": revenue_acc,
    "satisfaction_accuracy": satisfaction_acc
})
```

### ğŸ”„ ëª¨ë¸ ì„œë¹™

#### **1. FastAPI ì„œë²„**
```python
from fastapi import FastAPI
from HRM_Models import HRMActor, HRMPredictor

app = FastAPI()

# ëª¨ë¸ ë¡œë“œ
actor = HRMActor.load("./models/actor_best.pth")
revenue_predictor = HRMPredictor.load("./models/revenue_predictor.pth")

@app.post("/predict/action")
async def predict_action(state: dict):
    action = actor.predict(state)
    return {"recommended_action": action}

@app.post("/predict/revenue")
async def predict_revenue(state: dict):
    revenue = revenue_predictor.predict(state)
    return {"predicted_revenue": revenue}
```

#### **2. Docker ì»¨í…Œì´ë„ˆí™”**
```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## ğŸ”§ ë¬¸ì œ í•´ê²° (Troubleshooting)

### â— ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### **1. ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜**
```bash
# ì˜¤ë¥˜: CUDA out of memory
```
**í•´ê²°ì±…:**
```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
training_params['batch_size'] = 32  # ê¸°ë³¸ê°’: 128

# ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ì‚¬ìš©
training_params['gradient_accumulation_steps'] = 4

# ë©”ëª¨ë¦¬ ì •ë¦¬
torch.cuda.empty_cache()
```

#### **2. í•™ìŠµ ìˆ˜ë ´ ë¬¸ì œ**
```bash
# ì¦ìƒ: ë¦¬ì›Œë“œê°€ ê°œì„ ë˜ì§€ ì•ŠìŒ
```
**í•´ê²°ì±…:**
```python
# í•™ìŠµë¥  ì¡°ì •
optimizer_params['learning_rate'] = 1e-4  # ê¸°ë³¸ê°’: 1e-3

# íƒí—˜ ë¹„ìœ¨ ì¦ê°€
training_params['epsilon'] = 0.3  # ê¸°ë³¸ê°’: 0.1

# ê²½í—˜ ì¬ìƒ ë²„í¼ í¬ê¸° ì¦ê°€
training_params['memory_size'] = 100000  # ê¸°ë³¸ê°’: 50000
```

#### **3. ì„¤ì¹˜ ì˜¤ë¥˜**
```bash
# ì˜¤ë¥˜: torch ì„¤ì¹˜ ì‹¤íŒ¨
```
**í•´ê²°ì±…:**
```bash
# CUDA ë²„ì „ í™•ì¸
nvidia-smi

# PyTorch ì¬ì„¤ì¹˜ (CUDA 11.8 ê¸°ì¤€)
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### ğŸ› ë””ë²„ê¹… íŒ

#### **1. ìƒì„¸ ë¡œê¹… í™œì„±í™”**
```python
import logging
logging.basicConfig(level=logging.DEBUG)

# ë˜ëŠ” ì‹¤í–‰ ì‹œ
python HRM_Main.py --mode demo --debug --verbose
```

#### **2. í”„ë¡œíŒŒì¼ë§**
```python
import cProfile
import pstats

# ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
cProfile.run('trainer.run()', 'profile_stats')
stats = pstats.Stats('profile_stats')
stats.sort_stats('cumulative').print_stats(10)
```

#### **3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§**
```python
import psutil
import torch

def log_memory_usage():
    # CPU ë©”ëª¨ë¦¬
    cpu_memory = psutil.virtual_memory().percent
    
    # GPU ë©”ëª¨ë¦¬
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() * 100
        print(f"CPU: {cpu_memory:.1f}%, GPU: {gpu_memory:.1f}%")
```

### ğŸ“ ì§€ì› ë° ë„ì›€

#### **1. ì´ìŠˆ ë¦¬í¬íŒ…**
- GitHub Issues: [í”„ë¡œì íŠ¸ ì´ìŠˆ í˜ì´ì§€]
- ì´ë©”ì¼: support@hrm-framework.com
- ë””ìŠ¤ì½”ë“œ: [ì»¤ë®¤ë‹ˆí‹° ì„œë²„]

#### **2. FAQ**
**Q: í•™ìŠµì´ ë„ˆë¬´ ëŠë ¤ìš”.**
A: GPU ì‚¬ìš© í™•ì¸, ë°°ì¹˜ í¬ê¸° ì¡°ì •, ë¶„ì‚° í•™ìŠµ ê³ ë ¤

**Q: ì˜ˆì¸¡ ì •í™•ë„ê°€ ë‚®ì•„ìš”.**
A: ë” ë§ì€ ì—í”¼ì†Œë“œ í•™ìŠµ, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹, ë°ì´í„° í’ˆì§ˆ í™•ì¸

**Q: ë©”ëª¨ë¦¬ ì˜¤ë¥˜ê°€ ë°œìƒí•´ìš”.**
A: ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°, ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì‚¬ìš©, ë” í° ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì‚¬ìš©

---

## ğŸ¤ ê¸°ì—¬ ë°©ë²• (Contributing)

### ğŸ”„ ê°œë°œ í”„ë¡œì„¸ìŠ¤

#### **1. í¬í¬ ë° í´ë¡ **
```bash
# í¬í¬ í›„ í´ë¡ 
git clone https://github.com/your-username/hrm-learning-framework.git
cd hrm-learning-framework

# ê°œë°œ ë¸Œëœì¹˜ ìƒì„±
git checkout -b feature/new-feature
```

#### **2. ê°œë°œ í™˜ê²½ ì„¤ì •**
```bash
# ê°œë°œìš© ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements-dev.txt

# pre-commit í›… ì„¤ì¹˜
pre-commit install
```

#### **3. ì½”ë“œ ì‘ì„± ë° í…ŒìŠ¤íŠ¸**
```bash
# ì½”ë“œ ì‘ì„±
# ... ê°œë°œ ì‘ì—… ...

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python -m pytest tests/

# ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬
flake8 *.py
black *.py
```

#### **4. í’€ ë¦¬í€˜ìŠ¤íŠ¸**
```bash
# ë³€ê²½ì‚¬í•­ ì»¤ë°‹
git add .
git commit -m "feat: add new feature"

# í‘¸ì‹œ ë° PR ìƒì„±
git push origin feature/new-feature
# GitHubì—ì„œ Pull Request ìƒì„±
```

### ğŸ“‹ ê¸°ì—¬ ê°€ì´ë“œë¼ì¸

#### **1. ì½”ë“œ ìŠ¤íƒ€ì¼**
- **Python**: PEP 8 ì¤€ìˆ˜
- **í¬ë§·íŒ…**: Black ì‚¬ìš©
- **ì„í¬íŠ¸**: isort ì‚¬ìš©
- **íƒ€ì… íŒíŠ¸**: í•„ìˆ˜

#### **2. ì»¤ë°‹ ë©”ì‹œì§€**
```bash
# í˜•ì‹: type(scope): description
feat(models): add new prediction model
fix(trainer): resolve memory leak issue
docs(readme): update installation guide
test(utils): add unit tests for data processor
```

#### **3. í…ŒìŠ¤íŠ¸ ì‘ì„±**
```python
import pytest
from HRM_Models import HRMActor

def test_hrm_actor_initialization():
    """HRMActor ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
    actor = HRMActor(input_dim=50, hidden_dim=128, output_dim=10)
    assert actor.input_dim == 50
    assert actor.hidden_dim == 128
    assert actor.output_dim == 10

def test_hrm_actor_forward_pass():
    """HRMActor ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸"""
    actor = HRMActor(input_dim=50, hidden_dim=128, output_dim=10)
    input_tensor = torch.randn(32, 50)
    output = actor(input_tensor)
    assert output.shape == (32, 10)
```

### ğŸ¯ ê¸°ì—¬ ì˜ì—­

#### **1. ìš°ì„ ìˆœìœ„ ë†’ìŒ**
- ğŸ› **ë²„ê·¸ ìˆ˜ì •**: ê¸°ì¡´ ê¸°ëŠ¥ ì˜¤ë¥˜ í•´ê²°
- ğŸ“Š **ì„±ëŠ¥ ìµœì í™”**: í•™ìŠµ ì†ë„ ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ 
- ğŸ“– **ë¬¸ì„œí™”**: ì‚¬ìš©ë²• ë° API ë¬¸ì„œ ê°œì„ 
- ğŸ§ª **í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€**: ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë° í†µí•© í…ŒìŠ¤íŠ¸ ì¶”ê°€

#### **2. ìƒˆë¡œìš´ ê¸°ëŠ¥**
- ğŸ¯ **ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜**: DDPG, SAC, TD3 ë“± ì¶”ê°€
- ğŸ¢ **ì‚°ì—…ë³„ íŠ¹í™”**: ì œì¡°ì—…, ê¸ˆìœµì—…, ì„œë¹„ìŠ¤ì—… íŠ¹í™” ëª¨ë¸
- ğŸŒ **ë‹¤êµ­ì–´ ì§€ì›**: ë‹¤ì–‘í•œ ì–¸ì–´ ë° ì§€ì—­ ë²•ê·œ ì§€ì›
- ğŸ“± **ì›¹ ì¸í„°í˜ì´ìŠ¤**: ëŒ€ì‹œë³´ë“œ ë° ê´€ë¦¬ ë„êµ¬

#### **3. ì—°êµ¬ ë° ì‹¤í—˜**
- ğŸ”¬ **ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜**: Transformer, Graph Neural Network ì ìš©
- ğŸ“ˆ **ë©”íƒ€ ëŸ¬ë‹**: Few-shot learning, Transfer learning
- ğŸ² **ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”**: Bayesian Neural Network ì ìš©

---

## ğŸ“„ ë¼ì´ì„¼ìŠ¤ (License)

### MIT License

```
MIT License

Copyright (c) 2024 HRM Learning Framework

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

## ğŸ“ ì—°ë½ì²˜ ë° ì§€ì› (Contact & Support)

### ğŸŒ ê³µì‹ ë§í¬
- **GitHub**: https://github.com/your-org/hrm-learning-framework
- **ë¬¸ì„œ**: https://hrm-framework.readthedocs.io
- **ë°ëª¨**: https://demo.hrm-framework.com
- **ë¸”ë¡œê·¸**: https://blog.hrm-framework.com

### ğŸ“§ ì—°ë½ì²˜
- **ì¼ë°˜ ë¬¸ì˜**: info@hrm-framework.com
- **ê¸°ìˆ  ì§€ì›**: support@hrm-framework.com
- **ë¹„ì¦ˆë‹ˆìŠ¤**: business@hrm-framework.com
- **ë³´ì•ˆ ì´ìŠˆ**: security@hrm-framework.com

### ğŸ’¬ ì»¤ë®¤ë‹ˆí‹°
- **Discord**: [HRM Framework Community]
- **Slack**: [Workspace ë§í¬]
- **Reddit**: r/HRMFramework
- **LinkedIn**: [Company Page]

### ğŸ“š ì¶”ê°€ ìë£Œ
- **ë…¼ë¬¸**: "Deep Reinforcement Learning for Human Resource Management" (2024)
- **ë°œí‘œ ìë£Œ**: [SlideShare ë§í¬]
- **ë¹„ë””ì˜¤ íŠœí† ë¦¬ì–¼**: [YouTube ì±„ë„]
- **ì›¨ë¹„ë‚˜**: ë§¤ì›” ì²«ì§¸ ì£¼ ëª©ìš”ì¼ ì˜¤í›„ 2ì‹œ (KST)

---

## ğŸ‰ ê°ì‚¬ì˜ ë§

### ğŸ‘¥ ê¸°ì—¬ìë“¤
- **í•µì‹¬ ê°œë°œíŒ€**: [ê°œë°œì ëª©ë¡]
- **ì—°êµ¬íŒ€**: [ì—°êµ¬ì› ëª©ë¡]
- **ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬ì**: [ê¸°ì—¬ì ëª©ë¡]

### ğŸ™ íŠ¹ë³„ ê°ì‚¬
- **RL2LQS í”„ë¡œì íŠ¸**: ì›ë³¸ í”„ë ˆì„ì›Œí¬ ì œê³µ
- **PyTorch íŒ€**: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- **OpenAI**: ê°•í™”í•™ìŠµ ì—°êµ¬ ë° ì•Œê³ ë¦¬ì¦˜
- **ì»¤ë®¤ë‹ˆí‹°**: í”¼ë“œë°± ë° ë²„ê·¸ ë¦¬í¬íŠ¸

---

## ğŸ“ˆ ë¡œë“œë§µ (Roadmap)

### ğŸ¯ 2024 Q4
- âœ… ê¸°ë³¸ í”„ë ˆì„ì›Œí¬ ì™„ì„±
- âœ… Actor-Critic, PPO ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
- âœ… ì˜ˆì¸¡ ëª¨ë¸ (ìˆ˜ìµ, ë§Œì¡±ë„) êµ¬í˜„
- âœ… ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶•

### ğŸš€ 2025 Q1
- ğŸ”„ ì›¹ ì¸í„°í˜ì´ìŠ¤ ê°œë°œ
- ğŸ”„ API ì„œë²„ êµ¬ì¶•
- ğŸ”„ í´ë¼ìš°ë“œ ë°°í¬ ì§€ì›
- ğŸ”„ ë‹¤êµ­ì–´ ì§€ì› (ì˜ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´)

### ğŸŒŸ 2025 Q2
- ğŸ“‹ ì‚°ì—…ë³„ íŠ¹í™” ëª¨ë¸
- ğŸ“‹ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì§€ì›
- ğŸ“‹ ì—°í•© í•™ìŠµ (Federated Learning)
- ğŸ“‹ AutoML ê¸°ëŠ¥ ì¶”ê°€

### ğŸ”® 2025 Q3-Q4
- ğŸ“‹ Transformer ê¸°ë°˜ ëª¨ë¸
- ğŸ“‹ Graph Neural Network ì ìš©
- ğŸ“‹ ë©”íƒ€ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- ğŸ“‹ ì—£ì§€ ë””ë°”ì´ìŠ¤ ì§€ì›

---

**ğŸ‰ HRM Learning Framework - ì¸ê³µì§€ëŠ¥ìœ¼ë¡œ í˜ì‹ í•˜ëŠ” ì¸ì‚¬ê´€ë¦¬ì˜ ë¯¸ë˜! ğŸš€**

*"ì™„ë²½í•˜ê²Œ ì‘ë™í•˜ëŠ” ì½”ë“œë¡œ ì¡°ì§ì˜ ì„±ê³¼ë¥¼ ê·¹ëŒ€í™”í•˜ì„¸ìš”!"*
